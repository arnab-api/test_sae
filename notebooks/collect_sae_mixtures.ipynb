{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/sae/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-23 22:09:33 __main__ INFO     torch.__version__='2.4.1', torch.version.cuda='12.1'\n",
      "2024-10-23 22:09:33 __main__ INFO     torch.cuda.is_available()=True, torch.cuda.device_count()=1, torch.cuda.get_device_name()='NVIDIA RTX A6000'\n",
      "2024-10-23 22:09:33 __main__ INFO     transformers.__version__='4.44.2'\n"
     ]
    }
   ],
   "source": [
    "import time, json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "import spacy\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import os\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils\n",
    "from src import functional\n",
    "from datasets import load_dataset\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")\n",
    "logger.info(f\"{torch.cuda.is_available()=}, {torch.cuda.device_count()=}, {torch.cuda.get_device_name()=}\")\n",
    "logger.info(f\"{transformers.__version__=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-23 22:09:51 accelerate.utils.modeling INFO     We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-23 22:09:52 src.models INFO     loaded model </home/local_arnab/Codes/00_MODEL/allenai/OLMo-1B-0724-hf> | size: 4882.004 MB | dtype: torch.float32 | device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from nnsight import LanguageModel\n",
    "from src.models import ModelandTokenizer\n",
    "\n",
    "# model_name = \"openai-community/gpt2-xl\"\n",
    "# model_name = \"openai-community/gpt2\"\n",
    "# model_name = \"EleutherAI/pythia-410m\"\n",
    "# model_name = \"google/gemma-2-2b\"\n",
    "# model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "model_name = \"allenai/OLMo-1B-0724-hf\"\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=model_name,\n",
    "    torch_dtype=torch.float32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"roneneldan/TinyStories\"\n",
    "sae_data_checkpoint = 2000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/Codes/Projects/sae/notebooks/../dictionary_learning/dictionary.py:206: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = t.load(path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GatedAutoEncoder(\n",
       "  (encoder): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "  (decoder): Linear(in_features=16384, out_features=2048, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dictionary_learning.dictionary import AutoEncoder, GatedAutoEncoder\n",
    "\n",
    "model_data_dir = os.path.join(\n",
    "    model_name.split(\"/\")[-1],\n",
    "    dataset_name.split(\"/\")[-1],\n",
    ")\n",
    "\n",
    "sae_dir = os.path.join(\n",
    "    env_utils.DEFAULT_RESULTS_DIR,\n",
    "    \"train_sae\",\n",
    "    model_data_dir,\n",
    "    str(sae_data_checkpoint),\n",
    "    \"trainer_0/ae.pt\"\n",
    ")\n",
    "\n",
    "sae = GatedAutoEncoder.from_pretrained(\n",
    "    path = sae_dir,\n",
    "    device=mt.device\n",
    ").to(mt.dtype)\n",
    "sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-23 22:15:55 src.utils.experiment_utils INFO     setting all seeds to 123456\n",
      "2024-10-23 22:15:55 urllib3.connectionpool DEBUG    Resetting dropped connection: huggingface.co\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-23 22:15:56 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/jahjinx/IMDb_movie_reviews HTTP/11\" 200 1693\n",
      "2024-10-23 22:15:56 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "2024-10-23 22:15:56 urllib3.connectionpool DEBUG    https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/jahjinx/IMDb_movie_reviews/jahjinx/IMDb_movie_reviews.py HTTP/11\" 404 0\n",
      "2024-10-23 22:15:56 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/jahjinx/IMDb_movie_reviews HTTP/11\" 200 1693\n",
      "2024-10-23 22:15:56 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2024-10-23 22:15:56 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/jahjinx/IMDb_movie_reviews/resolve/ef30f6a046230c843d79822b928267efd9453d5b/README.md HTTP/11\" 200 0\n",
      "2024-10-23 22:15:56 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2024-10-23 22:15:56 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/jahjinx/IMDb_movie_reviews/resolve/ef30f6a046230c843d79822b928267efd9453d5b/.huggingface.yaml HTTP/11\" 404 0\n",
      "2024-10-23 22:15:56 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): datasets-server.huggingface.co:443\n",
      "2024-10-23 22:15:56 urllib3.connectionpool DEBUG    https://datasets-server.huggingface.co:443 \"GET /info?dataset=jahjinx/IMDb_movie_reviews HTTP/11\" 200 None\n",
      "2024-10-23 22:15:56 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/jahjinx/IMDb_movie_reviews/tree/ef30f6a046230c843d79822b928267efd9453d5b/data?recursive=False&expand=False HTTP/11\" 404 79\n",
      "2024-10-23 22:15:56 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2024-10-23 22:15:56 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/jahjinx/IMDb_movie_reviews/revision/ef30f6a046230c843d79822b928267efd9453d5b HTTP/11\" 200 1693\n",
      "2024-10-23 22:15:56 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2024-10-23 22:15:56 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/jahjinx/IMDb_movie_reviews/resolve/ef30f6a046230c843d79822b928267efd9453d5b/dataset_infos.json HTTP/11\" 404 0\n",
      "2024-10-23 22:15:56 filelock DEBUG    Attempting to acquire lock 139966075957904 on /home/local_arnab/.cache/huggingface/datasets/_home_local_arnab_.cache_huggingface_datasets_jahjinx___im_db_movie_reviews_default_0.0.0_ef30f6a046230c843d79822b928267efd9453d5b.lock\n",
      "2024-10-23 22:15:56 filelock DEBUG    Lock 139966075957904 acquired on /home/local_arnab/.cache/huggingface/datasets/_home_local_arnab_.cache_huggingface_datasets_jahjinx___im_db_movie_reviews_default_0.0.0_ef30f6a046230c843d79822b928267efd9453d5b.lock\n",
      "2024-10-23 22:15:56 fsspec.local DEBUG    open file: /home/local_arnab/.cache/huggingface/datasets/jahjinx___im_db_movie_reviews/default/0.0.0/ef30f6a046230c843d79822b928267efd9453d5b/dataset_info.json\n",
      "2024-10-23 22:15:56 filelock DEBUG    Attempting to release lock 139966075957904 on /home/local_arnab/.cache/huggingface/datasets/_home_local_arnab_.cache_huggingface_datasets_jahjinx___im_db_movie_reviews_default_0.0.0_ef30f6a046230c843d79822b928267efd9453d5b.lock\n",
      "2024-10-23 22:15:56 filelock DEBUG    Lock 139966075957904 released on /home/local_arnab/.cache/huggingface/datasets/_home_local_arnab_.cache_huggingface_datasets_jahjinx___im_db_movie_reviews_default_0.0.0_ef30f6a046230c843d79822b928267efd9453d5b.lock\n",
      "2024-10-23 22:15:56 filelock DEBUG    Attempting to acquire lock 139966620917328 on /home/local_arnab/.cache/huggingface/datasets/jahjinx___im_db_movie_reviews/default/0.0.0/ef30f6a046230c843d79822b928267efd9453d5b_builder.lock\n",
      "2024-10-23 22:15:56 filelock DEBUG    Lock 139966620917328 acquired on /home/local_arnab/.cache/huggingface/datasets/jahjinx___im_db_movie_reviews/default/0.0.0/ef30f6a046230c843d79822b928267efd9453d5b_builder.lock\n",
      "2024-10-23 22:15:56 fsspec.local DEBUG    open file: /home/local_arnab/.cache/huggingface/datasets/jahjinx___im_db_movie_reviews/default/0.0.0/ef30f6a046230c843d79822b928267efd9453d5b/dataset_info.json\n",
      "2024-10-23 22:15:56 filelock DEBUG    Attempting to release lock 139966620917328 on /home/local_arnab/.cache/huggingface/datasets/jahjinx___im_db_movie_reviews/default/0.0.0/ef30f6a046230c843d79822b928267efd9453d5b_builder.lock\n",
      "2024-10-23 22:15:56 filelock DEBUG    Lock 139966620917328 released on /home/local_arnab/.cache/huggingface/datasets/jahjinx___im_db_movie_reviews/default/0.0.0/ef30f6a046230c843d79822b928267efd9453d5b_builder.lock\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Beautifully photographed and ably acted, generally, but the writing is very slipshod. There are scenes of such unbelievability that there is no joy in the watching. The fact that the young lover has a twin brother, for instance, is so contrived that I groaned out loud. And the \"emotion-light bulb connection\" seems gimmicky, too.<br /><br />I don\\'t know, though. If you have a few glasses of wine and feel like relaxing with something pretty to look at with a few flaccid comedic scenes, this is a pretty good movie. No major effort on the part of the viewer required. But Italian film, especially Italian comedy, is usually much, much better than this.',\n",
       " 'Well, where to start describing this celluloid debacle? You already know the big fat NADA passing as a plot, so let\\'s jut point out that this is so PC it\\'s offensive. Hard to believe that Frank Oz, the same guy that gave us laugh riots like Little Shop of Horrors and Bowfinger, made this unfunny mess.<br /><br />So, this guy doesn\\'t know he\\'s gay till this actor points it out. OK, sure. If anyone ever says I\\'m gay, I\\'ll know the truth, even if I currently like girls more than George Luca$ likes a dollar.<br /><br />And how to know the true nature of my sexuality? Well, if I like classic litterature, dancing and Barbra Streisand, I\\'m gay. If I dress like a blind man in a hurry (with half my shirt hanging out), I\\'m straight. Oh, sure.<br /><br />And here\\'s the big cliché of clichés: no matter how you look, there\\'s always a very attractive Hollywood actor who\\'ll adore every bit of grease under your skin, or a top model who\\'ll love your zero IQ, your butt-ugly face and your pointing-out ears. If all those gay common places weren\\'t enough to get me angry, this did. In real world looks matter, folks, and I know for sure.<br /><br />I see it coming: now you\\'ll say \"Relax! It\\'s a comedy! Don\\'t take it so seriously!\". If being a comedy gives anything \"carte blanche\" to suck out loud, I think the world has a serious problem. Wouldn\\'t be much better (and funnier) to make a movie to denegate those old tiresome clichés, instead of magnifying them over and over again?<br /><br />So, one of the absolutely worst movies I\\'ve ever seen. 1 out of 10. If giving this rating has something to do with my sexual tendence, please let me know. I\\'m interested.',\n",
       " 'I first caught the movie on its first run on HBO in (probably) 1981 and being 15 years old I thought the movie was hilarious. I remember NOT seeing the Alfred E. Neuman depictions shown in the theatrical trailers. When MAD Magazine satired the movie and abruptly halted half way through with apologies from the\\xa0\"usual gang\" for lowering themselves to satire such a piece of crap, I just assumed they were poking fun at themselves, which I\\'m sure they were, but to seriously find them ( and Ron Liebman ) so embarrassed to remove their names from any credits, I was quite surprised. Surely there are many worse movies to be associated with. Watching the movie on video now (at age 32) with the MAD references restored, I still get a kick out of it. And being a Ron Liebman fan (Hot Rock, Where\\'s Poppa?) I think it\\'s his crown jewel of performances (SAY IT AGAAAAIN)',\n",
       " \"I love Umberto Lenzi's cop movies -- ROME ARMED TO THE TEETH is my favorite -- and his DESERT COMMANDOS pretty much legitimized the Italian Euro War phenomenon by managing to actually be a pretty good movie. Give him guns, machines, some guys to run around talking tough and it's hard for him to miss.<br /><br />What a shame it is then to encounter his EATEN ALIVE and CANNIBAL FEROX. They could have been by anybody, really, and one watches them with a sense that he was under contract, was told what kind of films to make, was assigned a cast & crew and given a budget, deadline and script. Lenzi then went out and executed his films the same way that a guy at McDonald's fixes a batch of French Fries. Of the two, EATEN ALIVE is the more original -- which is kind of amazing considering that it features extensive footage copped from three other cannibal movies -- and easier to enjoy than CANNIBAL FEROX, though not much.<br /><br />I will let others outline the plot: What fascinated me about the film is how staged it all looks, and yet what a somewhat infamous reputation this film has as some sort of all-out assault on good taste. It isn't, though the film is an exercise in bad taste, complete with a title-earning scene where two of the pretty supporting ladies are quite literally sliced up and eaten alive by cannibals which is the film's high point. The problem is that Lenzi lets us get a good, long look at the cannibal feast scene and it has the convincing ring of a 3rd season STAR TREK episode complete with a fake jungle set for the really gory close up shots. And I don't know about anyone else, but if someone was slicing off pieces of my person and eating them I wouldn't just lie there and look distressed.<br /><br />My problem with the film might be that I have a lot of respect for Lenzi as a filmmaker, and again this could have been directed by anybody. All of your cannibal movie conventions are touched on but it never feels like they are filmed with any real conviction, other than to try and bully Ruggero Deodato out of the sandbox. The two directors certainly must have known each other and either had a sort of juvenile rivalry or an actual dislike for each other's work and a conscious need to upstage them. Lenzi invented the cannibal genre with MAN FROM DEEP RIVER, Deodato blew it away with JUNGLE HOLOCAUST, Lenzi fired back with EATEN ALIVE, Deodato blew him (and everyone else) away with the superior CANNIBAL HOLOCAUST, and Lenzi fired a last parting salvo with CANNIBAL FEROX, which is about as realistic as that Krazy Glu commercial where the guy superglues his hardhat to a girder and hangs in midair.<br /><br />This film is less desperate and a bit more hesitant to push viewers into the abyss with everything holy than FEROX, which remains as a sort of misguided attempt to upstage CANNIBAL HOLOCAUST. This one is more of a mish-mash, with an interesting jungle adventure crossed with a Jim Jones like suicide cult -- the cannibals seem like they were added as an afterthought rather than the reason for making the film. I think that fans of the genre will have a better impression of the film than fans of Lenzi's films looking for something new by the formidable director. It isn't as entertaining as SLAVE OF THE CANNIBAL GOD, adventurous as his own MAN FROM DEEP RIVER and certainly lacks the wallop of CANNIBAL HOLOCAUST. Combine those considerations with the recycled footage, stomach churning scenes of animal violence, misogynistic scenes of sexual violence and stagy, wooden methodology of film-making and what you get ends up being an OK jungle thriller with two or three standout scenes, and Umberto Lenzi was capable of so much more.<br /><br />4/10; Gore freaks will go nuts however.\",\n",
       " 'I generally won\\'t review movies I haven\\'t seen in awhile, so I\\'ll pop them in or rent them to give a full and fresh take on the film. In the case of \\'A Sound of Thunder,\\' I remembered my vow of never seeing this movie ever again, so I\\'ll just go on memory. In fact, I haven\\'t thought of how badly made this movie was until I read someone else\\'s review and remembered the experience I had back in 2005, when I actually saw this in the theater. My movie buddy forced me to see it, though I wasn\\'t interested, and wow. (Later on, I forced him to see \\'Basic Instinct 2\\' in the theater, reminding him he made me see this crap. So, I guess that made us even.) I certainly had my share of deep laughs (at the movie\\'s expense, of course,) which didn\\'t make him happy as he really wanted to see it. The time-travel/butterfly effect film had so many bad graphics, the loudest chuckles from me was whenever they showed the dinosaur (God, I loved seeing that dino and them actually being scared of it \\x96 it was hilarious!) or just simply, Ben Kingsley. It\\'s great, Kingsley can remind us on how human actors can be: going from \\'Gandhi\\' and \\'Schindler\\'s List\\' to, uh, this. (Even a Meryl Streep can do a \\'She-Devil\\' from time to time, so they\\'re forgiven.) For months, I pulled an MST3k with my buddy, consistently referencing this movie to any low-rent sci-fi film or Kingsley flick. Yes, the movie would be a great movie to see drunk (or otherwise inebriated): horrible over-the-top acting, \"special\" FX that even the Nintendo64 would turn away and ridiculous plot twists. The biggest disappointment was that the Razzies didn\\'t even nominate this film for any award.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from src.utils import experiment_utils\n",
    "experiment_utils.set_seed(123456)\n",
    "\n",
    "# eval_dataset_name = \"mickume/harry_potter_tiny\"\n",
    "eval_dataset_name = \"jahjinx/IMDb_movie_reviews\"\n",
    "\n",
    "eval_dataset = load_dataset(eval_dataset_name)\n",
    "eval_dataset[\"train\"][:5][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:34,  2.86it/s]\n"
     ]
    }
   ],
   "source": [
    "relu = torch.nn.ReLU()\n",
    "\n",
    "cache_dir = os.path.join(\n",
    "    env_utils.DEFAULT_RESULTS_DIR,\n",
    "    \"cache_sae_mixtures\",\n",
    "    eval_dataset_name.split(\"/\")[-1],\n",
    "    model_data_dir,\n",
    "    str(sae_data_checkpoint),\n",
    ")\n",
    "\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "from src.models import prepare_input\n",
    "from src.functional import get_module_nnsight, free_gpu_cache\n",
    "\n",
    "limit = 100\n",
    "context_limit = 1024\n",
    "\n",
    "sae_layer_name = mt.layer_name_format.format(mt.n_layer // 2)\n",
    "\n",
    "for doc_index, doc in tqdm(enumerate(eval_dataset[\"train\"][:limit][\"text\"])):\n",
    "    inputs = prepare_input(\n",
    "        prompts = doc,\n",
    "        tokenizer = mt\n",
    "    )\n",
    "    if inputs[\"input_ids\"].shape[1] > context_limit:\n",
    "        inputs[\"input_ids\"] = inputs[\"input_ids\"][:, :context_limit]\n",
    "        inputs[\"attention_mask\"] = inputs[\"attention_mask\"][:, :context_limit]\n",
    "\n",
    "    # print(f\"{doc=}\")\n",
    "    # logger.info(inputs[\"input_ids\"].shape)\n",
    "\n",
    "    with mt.trace(inputs, scan = False, validate = False) as trace:\n",
    "        module = get_module_nnsight(mt, sae_layer_name)\n",
    "        sae_input = module.output[0].save()\n",
    "    \n",
    "    sae_mixture = sae.encode(sae_input)\n",
    "    # logger.info(f\"{sae_input.shape=} | {sae_mixture.shape=}\")\n",
    "\n",
    "    cache = {\n",
    "        \"layer\": sae_layer_name,\n",
    "        \"doc\": doc,\n",
    "        \"sae_input\": sae_input.detach().cpu().numpy().astype(np.float32),\n",
    "        \"sae_mixture\": sae_mixture.detach().cpu().numpy().astype(np.float32),\n",
    "    }\n",
    "\n",
    "    cache_path = os.path.join(cache_dir, f\"{doc_index}\")\n",
    "    np.savez_compressed(cache_path, **cache)\n",
    "\n",
    "    free_gpu_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 162, 16384), (1, 162, 2048))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "sae_path = \"/home/local_arnab/Codes/Projects/sae/results/cache_sae_mixtures/IMDb_movie_reviews/OLMo-1B-0724-hf/TinyStories/2000000/39.npz\"\n",
    "\n",
    "file = np.load(sae_path)\n",
    "file[\"sae_mixture\"].shape, file[\"sae_input\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import ReLU\n",
    "relu = ReLU()\n",
    "relu(torch.Tensor(file[\"sae_mixture\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([162, 16384])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.Tensor(file[\"sae_mixture\"]).squeeze()\n",
    "t.shape\n",
    "# t.mean(dim = 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1501.8020)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeysView(NpzFile '/home/local_arnab/Codes/Projects/sae/results/cache_sae_mixtures/IMDb_movie_reviews/OLMo-1B-0724-hf/TinyStories/2000000/39.npz' with keys: layer, doc, sae_input, sae_mixture)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.keys()  # Check the keys in the loaded file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
