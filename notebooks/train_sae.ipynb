{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/sae/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('2.4.1', '4.44.2', '12.1')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import baukit\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import os\n",
    "from src import functional\n",
    "import numpy as np\n",
    "import logging\n",
    "from src import models\n",
    "\n",
    "from src.utils import logging_utils\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "torch.__version__, transformers.__version__, torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-16 11:10:30 numexpr.utils INFO     Note: NumExpr detected 24 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2024-10-16 11:10:30 numexpr.utils INFO     NumExpr defaulting to 8 threads.\n",
      "2024-10-16 11:10:30 datasets INFO     PyTorch version 2.4.1 available.\n",
      "2024-10-16 11:10:30 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2024-10-16 11:10:30 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/wikimedia/wikipedia HTTP/11\" 200 164229\n",
      "2024-10-16 11:10:30 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "2024-10-16 11:10:30 urllib3.connectionpool DEBUG    https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/wikimedia/wikipedia/wikimedia/wikipedia.py HTTP/11\" 404 0\n",
      "2024-10-16 11:10:30 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/wikimedia/wikipedia HTTP/11\" 200 164229\n",
      "2024-10-16 11:10:30 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2024-10-16 11:10:30 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/wikimedia/wikipedia/resolve/b04c8d1ceb2f5cd4588862100d08de323dccfbaa/README.md HTTP/11\" 200 0\n",
      "2024-10-16 11:10:31 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2024-10-16 11:10:31 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/wikimedia/wikipedia/resolve/b04c8d1ceb2f5cd4588862100d08de323dccfbaa/.huggingface.yaml HTTP/11\" 404 0\n",
      "2024-10-16 11:10:31 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): datasets-server.huggingface.co:443\n",
      "2024-10-16 11:10:31 urllib3.connectionpool DEBUG    https://datasets-server.huggingface.co:443 \"GET /info?dataset=wikimedia/wikipedia HTTP/11\" 200 None\n",
      "2024-10-16 11:10:31 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/wikimedia/wikipedia/revision/b04c8d1ceb2f5cd4588862100d08de323dccfbaa HTTP/11\" 200 164229\n",
      "2024-10-16 11:10:31 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/wikimedia/wikipedia/tree/b04c8d1ceb2f5cd4588862100d08de323dccfbaa?recursive=False&expand=False HTTP/11\" 200 32709\n",
      "2024-10-16 11:10:31 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"POST /api/datasets/wikimedia/wikipedia/paths-info/b04c8d1ceb2f5cd4588862100d08de323dccfbaa HTTP/11\" 200 236\n",
      "2024-10-16 11:10:31 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/wikimedia/wikipedia/tree/b04c8d1ceb2f5cd4588862100d08de323dccfbaa/20231101.ab?recursive=False&expand=False HTTP/11\" 200 245\n",
      "2024-10-16 11:10:31 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2024-10-16 11:10:31 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/wikimedia/wikipedia/revision/b04c8d1ceb2f5cd4588862100d08de323dccfbaa HTTP/11\" 200 164229\n",
      "2024-10-16 11:10:31 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2024-10-16 11:10:32 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/wikimedia/wikipedia/resolve/b04c8d1ceb2f5cd4588862100d08de323dccfbaa/dataset_infos.json HTTP/11\" 404 0\n",
      "2024-10-16 11:10:32 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"POST /api/datasets/wikimedia/wikipedia/paths-info/b04c8d1ceb2f5cd4588862100d08de323dccfbaa HTTP/11\" 200 244\n",
      "2024-10-16 11:10:32 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/wikimedia/wikipedia/tree/b04c8d1ceb2f5cd4588862100d08de323dccfbaa/20231101.en?recursive=False&expand=False HTTP/11\" 200 10169\n",
      "2024-10-16 11:10:32 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2024-10-16 11:10:32 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2024-10-16 11:10:32 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2024-10-16 11:10:32 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2024-10-16 11:10:32 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2024-10-16 11:10:32 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2024-10-16 11:10:32 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2024-10-16 11:10:32 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2024-10-16 11:10:32 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2024-10-16 11:10:32 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2024-10-16 11:10:32 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2024-10-16 11:10:32 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2024-10-16 11:10:32 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2024-10-16 11:10:32 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2024-10-16 11:10:32 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/wikimedia/wikipedia/revision/b04c8d1ceb2f5cd4588862100d08de323dccfbaa HTTP/11\" 200 164229\n",
      "2024-10-16 11:10:32 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/wikimedia/wikipedia/revision/b04c8d1ceb2f5cd4588862100d08de323dccfbaa HTTP/11\" 200 164229\n",
      "2024-10-16 11:10:32 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/wikimedia/wikipedia/revision/b04c8d1ceb2f5cd4588862100d08de323dccfbaa HTTP/11\" 200 164229\n",
      "2024-10-16 11:10:32 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/wikimedia/wikipedia/revision/b04c8d1ceb2f5cd4588862100d08de323dccfbaa HTTP/11\" 200 164229\n",
      "2024-10-16 11:10:32 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/wikimedia/wikipedia/revision/b04c8d1ceb2f5cd4588862100d08de323dccfbaa HTTP/11\" 200 164229\n",
      "2024-10-16 11:10:32 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/wikimedia/wikipedia/revision/b04c8d1ceb2f5cd4588862100d08de323dccfbaa HTTP/11\" 200 164229\n",
      "2024-10-16 11:10:32 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/wikimedia/wikipedia/revision/b04c8d1ceb2f5cd4588862100d08de323dccfbaa HTTP/11\" 200 164229\n",
      "2024-10-16 11:10:32 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/wikimedia/wikipedia/revision/b04c8d1ceb2f5cd4588862100d08de323dccfbaa HTTP/11\" 200 164229\n",
      "2024-10-16 11:10:32 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/wikimedia/wikipedia/revision/b04c8d1ceb2f5cd4588862100d08de323dccfbaa HTTP/11\" 200 164229\n",
      "2024-10-16 11:10:32 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/wikimedia/wikipedia/revision/b04c8d1ceb2f5cd4588862100d08de323dccfbaa HTTP/11\" 200 164229\n",
      "2024-10-16 11:10:32 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/wikimedia/wikipedia/revision/b04c8d1ceb2f5cd4588862100d08de323dccfbaa HTTP/11\" 200 164229\n",
      "2024-10-16 11:10:32 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/wikimedia/wikipedia/revision/b04c8d1ceb2f5cd4588862100d08de323dccfbaa HTTP/11\" 200 164229\n",
      "2024-10-16 11:10:32 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/wikimedia/wikipedia/revision/b04c8d1ceb2f5cd4588862100d08de323dccfbaa HTTP/11\" 200 164229\n",
      "2024-10-16 11:10:32 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/wikimedia/wikipedia/revision/b04c8d1ceb2f5cd4588862100d08de323dccfbaa HTTP/11\" 200 164229\n",
      "2024-10-16 11:10:32 filelock DEBUG    Attempting to acquire lock 139805693636816 on /home/local_arnab/.cache/huggingface/datasets/_home_local_arnab_.cache_huggingface_datasets_wikimedia___wikipedia_20231101.en_0.0.0_b04c8d1ceb2f5cd4588862100d08de323dccfbaa.lock\n",
      "2024-10-16 11:10:32 filelock DEBUG    Lock 139805693636816 acquired on /home/local_arnab/.cache/huggingface/datasets/_home_local_arnab_.cache_huggingface_datasets_wikimedia___wikipedia_20231101.en_0.0.0_b04c8d1ceb2f5cd4588862100d08de323dccfbaa.lock\n",
      "2024-10-16 11:10:32 fsspec.local DEBUG    open file: /home/local_arnab/.cache/huggingface/datasets/wikimedia___wikipedia/20231101.en/0.0.0/b04c8d1ceb2f5cd4588862100d08de323dccfbaa/dataset_info.json\n",
      "2024-10-16 11:10:32 filelock DEBUG    Attempting to release lock 139805693636816 on /home/local_arnab/.cache/huggingface/datasets/_home_local_arnab_.cache_huggingface_datasets_wikimedia___wikipedia_20231101.en_0.0.0_b04c8d1ceb2f5cd4588862100d08de323dccfbaa.lock\n",
      "2024-10-16 11:10:32 filelock DEBUG    Lock 139805693636816 released on /home/local_arnab/.cache/huggingface/datasets/_home_local_arnab_.cache_huggingface_datasets_wikimedia___wikipedia_20231101.en_0.0.0_b04c8d1ceb2f5cd4588862100d08de323dccfbaa.lock\n",
      "2024-10-16 11:10:32 filelock DEBUG    Attempting to acquire lock 139805686663888 on /home/local_arnab/.cache/huggingface/datasets/wikimedia___wikipedia/20231101.en/0.0.0/b04c8d1ceb2f5cd4588862100d08de323dccfbaa_builder.lock\n",
      "2024-10-16 11:10:32 filelock DEBUG    Lock 139805686663888 acquired on /home/local_arnab/.cache/huggingface/datasets/wikimedia___wikipedia/20231101.en/0.0.0/b04c8d1ceb2f5cd4588862100d08de323dccfbaa_builder.lock\n",
      "2024-10-16 11:10:32 fsspec.local DEBUG    open file: /home/local_arnab/.cache/huggingface/datasets/wikimedia___wikipedia/20231101.en/0.0.0/b04c8d1ceb2f5cd4588862100d08de323dccfbaa/dataset_info.json\n",
      "2024-10-16 11:10:32 filelock DEBUG    Attempting to release lock 139805686663888 on /home/local_arnab/.cache/huggingface/datasets/wikimedia___wikipedia/20231101.en/0.0.0/b04c8d1ceb2f5cd4588862100d08de323dccfbaa_builder.lock\n",
      "2024-10-16 11:10:32 filelock DEBUG    Lock 139805686663888 released on /home/local_arnab/.cache/huggingface/datasets/wikimedia___wikipedia/20231101.en/0.0.0/b04c8d1ceb2f5cd4588862100d08de323dccfbaa_builder.lock\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wiki = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(wiki[\"train\"]))\n",
    "# wiki[\"train\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiny = load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(tiny[\"train\"]))\n",
    "# tiny[\"train\"][5055]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-16 11:10:34 accelerate.utils.modeling INFO     We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2024-10-16 11:10:36 src.models INFO     loaded model </home/local_arnab/Codes/00_MODEL/Qwen/Qwen2.5-1.5B> | size: 9472.810 MB | dtype: torch.float32 | device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from src.models import ModelandTokenizer\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "# model_name = \"openai-community/gpt2-xl\"\n",
    "# model_name = \"openai-community/gpt2\"\n",
    "# model_name = \"EleutherAI/pythia-410m\"\n",
    "# model_name = \"google/gemma-2-2b\"\n",
    "# model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "model_name = \"Qwen/Qwen2.5-1.5B\"\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=model_name,\n",
    "    # torch_dtype=torch.float16,\n",
    "    torch_dtype=torch.float32,\n",
    ")\n",
    "\n",
    "# lm = LanguageModel(\n",
    "#     model_key=model_name,\n",
    "#     # torch_dtype=torch.float16,\n",
    "#     device_map = \"auto\",\n",
    "#     dispatch=True\n",
    "# )\n",
    "# lm.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # layer_name_format = 'gpt_neox.layers.{}'\n",
    "# layer_name_format = mt.layer_name_format\n",
    "# n_embd = mt.n_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mt.embedder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.functional import get_module_nnsight\n",
    "# from src.functional import prepare_input\n",
    "\n",
    "# prompts = [\n",
    "#     \"A quick brown fox jumps over the lazy\",\n",
    "#     \"The capital of France is\",\n",
    "# ]    \n",
    "# tokens = prepare_input(prompts, tokenizer=mt.tokenizer)\n",
    "\n",
    "# tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mt.tokenizer.padding_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_tokens = prepare_input(\n",
    "#     prompts, tokenizer=mt.tokenizer,\n",
    "#     # truncation=True, max_length=5,\n",
    "#     padding_side=\"right\",\n",
    "# )\n",
    "\n",
    "# test_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_tokens = prepare_input(\n",
    "#     prompts, tokenizer=mt.tokenizer,\n",
    "#     truncation=True, max_length=5,\n",
    "#     # padding_side=\"left\",\n",
    "# )\n",
    "\n",
    "# test_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.inference_mode():\n",
    "#     with mt.trace(test_tokens) as tr:\n",
    "#         resid_out = get_module_nnsight(mt, layer_name_format.format(3)).output[0].save()\n",
    "#         # input = get_module_nnsight(mt, mt.embedder_name).input.save()\n",
    "#         # resid_in = get_module_nnsight(mt, layer_name_format.format(3)).input.save()\n",
    "#         # output = mt.output.save()\n",
    "\n",
    "# print(resid_out.shape)\n",
    "# # resid_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.inference_mode():\n",
    "#     with mt.trace(\n",
    "#         prompts,\n",
    "#         invoker_args={\"truncation\": True, \"max_length\": 5}\n",
    "#     ) as tr:\n",
    "#         resid_out_2 = get_module_nnsight(mt, layer_name_format.format(3)).output[0].save()\n",
    "#         # input = get_module_nnsight(mt, mt.embedder_name).input.save()\n",
    "#         # resid_in = get_module_nnsight(mt, layer_name_format.format(3)).input.save()\n",
    "#         # output = mt.output.save()\n",
    "\n",
    "# print(resid_out_2.shape)\n",
    "# # resid_out_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.allclose(resid_out, resid_out_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt.n_embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init self.activations.shape=torch.Size([0, 1536])\n",
      "init self.read.shape=torch.Size([0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d_submodule': 1536,\n",
       " 'io': 'out',\n",
       " 'n_ctxs': 26,\n",
       " 'ctx_len': 256,\n",
       " 'refresh_batch_size': 26,\n",
       " 'out_batch_size': 8192,\n",
       " 'device': 'cuda:0'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dictionary_learning import ActivationBuffer\n",
    "from dictionary_learning.training import trainSAE\n",
    "from src.functional import get_module_nnsight\n",
    "\n",
    "submodule = get_module_nnsight(mt, mt.layer_name_format.format(6))\n",
    "activation_dim = mt.n_embd\n",
    "dictionary_dim = 16384\n",
    "\n",
    "# data_iter = iter(tiny[\"train\"][:2100000][\"text\"])\n",
    "# data_iter = iter(tiny[\"train\"][:10000][\"text\"])\n",
    "data_iter = iter(wiki[\"train\"][:1200][\"text\"])\n",
    "\n",
    "\n",
    "tiny_buffer = ActivationBuffer(\n",
    "    data_iter,\n",
    "    mt,\n",
    "    submodule,\n",
    "    d_submodule=activation_dim, \n",
    "    n_ctxs=26, \n",
    "    ctx_len=256,\n",
    "    \n",
    "    refresh_batch_size=26,\n",
    "    device='cuda:0', \n",
    ")\n",
    "\n",
    "tiny_buffer.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter=1\n",
      "counter=2\n",
      "counter=3\n",
      "counter=4\n",
      "counter=5\n",
      "counter=6\n",
      "counter=7\n",
      "counter=8\n",
      "counter=9\n",
      "counter=10\n",
      "counter=11\n",
      "counter=12\n",
      "counter=13\n",
      "counter=14\n",
      "counter=15\n",
      "counter=16\n",
      "counter=17\n",
      "counter=18\n",
      "counter=19\n",
      "counter=20\n",
      "counter=21\n",
      "counter=22\n",
      "counter=23\n",
      "counter=24\n",
      "counter=25\n",
      "counter=26\n",
      "counter=27\n",
      "counter=28\n",
      "counter=29\n",
      "counter=30\n",
      "counter=31\n",
      "counter=32\n",
      "counter=33\n",
      "counter=34\n",
      "counter=35\n",
      "counter=36\n",
      "counter=37\n",
      "counter=38\n",
      "counter=39\n",
      "counter=40\n",
      "counter=41\n",
      "counter=42\n",
      "counter=43\n",
      "counter=44\n",
      "counter=45\n",
      "counter=46\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = 0\n",
    "while True:\n",
    "    try:\n",
    "        tiny_buffer.text_batch()\n",
    "        counter += 1\n",
    "        print(f\"{counter=}\")\n",
    "    except StopIteration:\n",
    "        break\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functional import free_gpu_cache\n",
    "\n",
    "free_gpu_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf trained_saes\n",
    "! rm -rf wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-16 10:31:03 src.utils.experiment_utils INFO     setting all seeds to 1234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['activation_dim', 'dict_size', 'lr', 'warmup_steps', 'resample_steps', 'seed', 'wandb_name', 'lm_name', 'layer', 'submodule_name']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "28it [01:12,  2.59s/it]\n"
     ]
    }
   ],
   "source": [
    "from dictionary_learning.trainers import TrainerTopK, GatedSAETrainer, GatedAnnealTrainer, StandardTrainer\n",
    "from dictionary_learning.dictionary import AutoEncoder\n",
    "from src.utils.experiment_utils import set_seed\n",
    "\n",
    "set_seed(1234)\n",
    "\n",
    "# train the sparse autoencoder (SAE)\n",
    "ae = trainSAE(\n",
    "    data = tiny_buffer,\n",
    "    trainer_configs= [{\n",
    "        'trainer' : GatedSAETrainer,\n",
    "        # 'dict_class' : AutoEncoder,\n",
    "        'activation_dim' : activation_dim,\n",
    "        'dict_size' : dictionary_dim,\n",
    "        'lr' : 1e-3,\n",
    "        # 'l1_penalty' : 1e-2,\n",
    "        'warmup_steps' : 3,\n",
    "        'resample_steps' : None,\n",
    "        'seed' : None,\n",
    "        'wandb_name' : 'GatedSAETrainer',\n",
    "        'lm_name': mt.name,\n",
    "        'layer': submodule,\n",
    "        'submodule_name': \"residual\",\n",
    "    }],\n",
    "    save_dir=\"trained_saes\",\n",
    "    save_steps=10,\n",
    "\n",
    "    # use_wandb=True,\n",
    "    # wandb_entity=\"dl-homeworks\",\n",
    "    # wandb_project=\"test_sae\",\n",
    "    # log_steps=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/Codes/Projects/sae/notebooks/../dictionary_learning/dictionary.py:206: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = t.load(path)\n"
     ]
    }
   ],
   "source": [
    "from dictionary_learning.dictionary import AutoEncoder, GatedAutoEncoder\n",
    "from dictionary_learning.trainers.top_k import AutoEncoderTopK\n",
    "from src.utils import env_utils\n",
    "\n",
    "ae_0 = GatedAutoEncoder.from_pretrained(\n",
    "    path = os.path.join(\n",
    "        env_utils.DEFAULT_RESULTS_DIR,\n",
    "        \"train_sae\",\n",
    "        \"Qwen2.5-1.5B\",\n",
    "        \"wikipedia\",\n",
    "        str(10000),\n",
    "        \"trainer_0/checkpoints/ae_0.pt\"\n",
    "    ),\n",
    "    # k = 100,\n",
    "    device='cuda:0'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GatedAutoEncoder(\n",
       "  (encoder): Linear(in_features=1536, out_features=16384, bias=False)\n",
       "  (decoder): Linear(in_features=16384, out_features=1536, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0002,  0.0162, -0.0010,  ..., -0.0243, -0.0069, -0.0150],\n",
       "        [-0.0141,  0.0139,  0.0003,  ..., -0.0059, -0.0178,  0.0090],\n",
       "        [-0.0002, -0.0115, -0.0247,  ...,  0.0135,  0.0073,  0.0149],\n",
       "        ...,\n",
       "        [ 0.0206, -0.0046,  0.0235,  ...,  0.0177,  0.0241, -0.0002],\n",
       "        [-0.0016,  0.0002,  0.0163,  ...,  0.0073, -0.0092, -0.0223],\n",
       "        [-0.0065, -0.0244, -0.0111,  ...,  0.0114, -0.0159, -0.0097]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae_0.encoder.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_1 = GatedAutoEncoder.from_pretrained(\n",
    "    path = os.path.join(\n",
    "        env_utils.DEFAULT_RESULTS_DIR,\n",
    "        \"train_sae\",\n",
    "        \"Qwen2.5-1.5B\",\n",
    "        \"wikipedia\",\n",
    "        str(10000),\n",
    "        \"trainer_0/checkpoints/ae_60.pt\"\n",
    "    ),\n",
    "    # k = 100,\n",
    "    device='cuda:0'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.9945e-04,  1.6562e-02, -6.1725e-04,  ..., -2.3844e-02,\n",
       "         -7.3131e-03, -1.4618e-02],\n",
       "        [-1.4073e-02,  1.3654e-02, -2.5202e-05,  ..., -5.1585e-03,\n",
       "         -1.7493e-02,  9.6451e-03],\n",
       "        [-6.2147e-04, -1.1176e-02, -2.4371e-02,  ...,  1.3880e-02,\n",
       "          6.9265e-03,  1.5251e-02],\n",
       "        ...,\n",
       "        [ 2.0845e-02, -4.1893e-03,  2.3065e-02,  ...,  1.8006e-02,\n",
       "          2.3994e-02, -6.1753e-05],\n",
       "        [-1.3185e-03, -8.2879e-05,  1.5939e-02,  ...,  6.9403e-03,\n",
       "         -8.9061e-03, -2.2619e-02],\n",
       "        [-6.1018e-03, -2.4697e-02, -1.1541e-02,  ...,  1.0957e-02,\n",
       "         -1.5486e-02, -1.0146e-02]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae_1.encoder.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(ae_0.encoder.weight, ae_1.encoder.weight, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
